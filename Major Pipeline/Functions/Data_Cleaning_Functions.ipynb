{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f302041",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d7e079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scraped_data(user_list, project_location):\n",
    "    # get_scraped_data function begins\n",
    "\n",
    "    # Process done by the function\n",
    "    scraped_data = pd.DataFrame()\n",
    "\n",
    "    for user in user_list:\n",
    "\n",
    "        input_path = f\"{project_location}Scraped Data\\\\{user}_data.csv\"\n",
    "\n",
    "\n",
    "        try:\n",
    "            user_df = pd.read_csv(input_path, low_memory = False)\n",
    "            print(\"Data from\", user, \"succesfully loaded with\", len(user_df), \"entries!\")\n",
    "\n",
    "\n",
    "        except FileNotFoundError:\n",
    "\n",
    "            user_df = pd.DataFrame()\n",
    "\n",
    "        scraped_data = pd.concat([scraped_data, user_df])\n",
    "\n",
    "    initial_length = len(scraped_data)\n",
    "    print(\"\")\n",
    "    print(\"Concatenated DF created with\", initial_length, \"entries!\")\n",
    "    \n",
    "    return scraped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8179bfca",
   "metadata": {},
   "source": [
    "# Entry Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "774c5803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_non_standard_values(df, column_dict):\n",
    "    # count_non_standard_values function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "    for curr_column in column_dict.keys():\n",
    "\n",
    "        non_standard_values_count = df[curr_column].apply(lambda x: x not in column_dict[curr_column]).sum()\n",
    "        \n",
    "        print(\"Non Standard Values in \" + str(curr_column) + \": \" + str(non_standard_values_count))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7834e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_from_rows_with_shifted_columns(df, allowance_dict = {\"Umweltplakette\" : [\"1 (Keine)\", \"4 (Grün)\", \"3 (Gelb)\", \"2 (Rot)\", np.nan],\n",
    "    # clean_from_rows_with_shifted_columns function begins\n",
    "                 \"Antriebsart\" : ['Heck', 'Front', 'Allrad', np.nan],\n",
    "                 \"Lackierung\" : ['Metallic', 'Andere', 'Vollleder', 'Enthalten', 'Keine Angabe', np.nan],\n",
    "                 \"Türen\" : ['2.0', '5.0', np.nan, '4.0', '3.0', '6.0', '1.0', '4', '5', '2222/BLN', '2222/BLT', '2', '3', '6', '9.0'],\n",
    "                                                              \"Scheckheftgepflegt\" : [\"Ja\", \"Nein\", np.nan]}):\n",
    "    \n",
    "    # Process done by the function\n",
    "    df1 = df.copy()\n",
    "    previous_length = len(df1)\n",
    "\n",
    "    for curr_feature in allowance_dict.keys():\n",
    "\n",
    "        allowed_values = allowance_dict[curr_feature]\n",
    "        df1 = df1[df1[curr_feature].isin(allowed_values)]\n",
    "\n",
    "    new_length = len(df1)\n",
    "    \n",
    "    print(\"Dropped\", previous_length - new_length, \"entries due to shifted columns!\")\n",
    "    \n",
    "    return df1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8280d1e2",
   "metadata": {},
   "source": [
    "# Column Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02641a62",
   "metadata": {},
   "source": [
    "Provide a function that shows the amount of missing data in every column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d39b8109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_info(df):\n",
    "    # missing_values_info function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "    df1 = df.copy()\n",
    "    # Count missing values in each column\n",
    "    missing_counts = df1.isna().sum()\n",
    "\n",
    "    # Calculate percentage of missing values in each column\n",
    "    total_rows = len(df1)\n",
    "    missing_percentages = np.round(missing_counts / total_rows, 3) * 100\n",
    "\n",
    "    # Create a DataFrame to store missing values information\n",
    "    missing_info_df = pd.DataFrame({\n",
    "        'Missing Values': missing_counts,\n",
    "        '% of Total Values': missing_percentages\n",
    "    })\n",
    "    missing_info_df = missing_info_df.sort_values([\"Missing Values\"], ascending = False)\n",
    "    \n",
    "    return missing_info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0f5c4b",
   "metadata": {},
   "source": [
    "## Dropping Columns that contain \"Unnamed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a35406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unnamed_columns(df):\n",
    "    # drop_unnamed_columns function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "    df1 = df.copy()\n",
    "    # List comprehension to get column names containing \"Unnamed\" \n",
    "    unnamed_columns = [col for col in df.columns if 'Unnamed' in col]\n",
    "    \n",
    "    print(\"Dropping \" + str(len(unnamed_columns)) + \" columns:\\n\" + str(unnamed_columns))\n",
    "    # Drop the unnamed columns\n",
    "    df1 = df1.drop(columns=unnamed_columns, errors = 'ignore')\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0f3eb0",
   "metadata": {},
   "source": [
    "## Dropping unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af71c9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unwanted_columns(df, columns_to_drop):\n",
    "    # drop_unwanted_columns function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "    df1 = df.copy()\n",
    "    df1 = df1.drop(columns = columns_to_drop, errors = \"ignore\")\n",
    "    print(\"Dropped \" + str(len(columns_to_drop)) + \" columns:\\n\" + str(columns_to_drop))\n",
    "          \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dfe834",
   "metadata": {},
   "source": [
    "## Merge Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9946f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns(df, column1, column2):\n",
    "    # merge_columns function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "    df1 = df.copy()\n",
    "    df1[column1] = df1[column1].fillna(df1[column2])\n",
    "    df1.drop(columns = [column2], inplace = True)\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be01979a",
   "metadata": {},
   "source": [
    "# Converting Strings to Floats/Integers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2706d341",
   "metadata": {},
   "source": [
    "## Convert Strings to other Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7916ab",
   "metadata": {},
   "source": [
    "### Single Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd211d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_values_single(df, column_name, replacement_dict):\n",
    "    # replace_values_single function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "    df1 = df.copy()\n",
    "    \n",
    "    for curr_value in replacement_dict.keys():\n",
    "        \n",
    "        \n",
    "        df1[column_name] = df1[column_name].str.replace(curr_value, replacement_dict[curr_value], regex = False)\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2b9d7e",
   "metadata": {},
   "source": [
    "### Multiple Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb1da69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_values_multiple(df, replacement_dict):\n",
    "    # replace_values_multiple function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "    df1 = df.copy()\n",
    "    \n",
    "    for column_name, replacements in replacement_dict.items():\n",
    "        for curr_value, replacement in replacements.items():\n",
    "            df1[column_name] = df1[column_name].str.replace(curr_value, replacement, regex = False)\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064ee6e4",
   "metadata": {},
   "source": [
    "### Remove leading and trailing whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc2f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_trailing_whitespace(df, column_names):\n",
    "    # remove_trailing_whitespace function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "    df1 = df.copy()\n",
    "    \n",
    "    for curr_column_name in column_names:\n",
    "        \n",
    "        df1[curr_column_name] = df1[curr_column_name].str.strip()\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8f86ed",
   "metadata": {},
   "source": [
    "### Replace Kraftstoff String with the most prominent Kraftstoff available in the initial Kraftstoff String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5236e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_fuel_type(fuel_type):\n",
    "    # classify_fuel_type function begins\n",
    "    if not isinstance(fuel_type, str):\n",
    "        return 'Other'\n",
    "    \n",
    "    # Process done by the function\n",
    "    fuel_type = fuel_type.lower()\n",
    "\n",
    "    keywords = {\n",
    "        'Diesel': ['diesel', 'biodiesel', 'pflanzenöl'],\n",
    "        'Gas': ['autogas', 'lpg', 'flüssiggas', 'erdgas', 'cng', 'biogas'],\n",
    "        'Super': ['super', 'super plus', 'benzin', 'e10', 'ethanol']\n",
    "    }\n",
    "\n",
    "    # Create a counter for all matches\n",
    "    counter = Counter()\n",
    "\n",
    "    for category, words in keywords.items():\n",
    "        for word in words:\n",
    "            if word in fuel_type:\n",
    "                counter[category] += fuel_type.count(word)\n",
    "    \n",
    "    # If the counter is not empty, return the most common category\n",
    "    if counter:\n",
    "        return counter.most_common(1)[0][0]\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "    \n",
    "# Apply the function to a DataFrame column\n",
    "def transform_fuel_types(df, column_name):\n",
    "    # transform_fuel_types function begins\n",
    "    df1 = df.copy()\n",
    "    df1[column_name] = df1[column_name].apply(classify_fuel_type)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9437fd",
   "metadata": {},
   "source": [
    "## Split Strings and turn to Floats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3f4b16",
   "metadata": {},
   "source": [
    "### Simple Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1c2278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string_to_1integer(df, split_dict):\n",
    "    # split_string_to_1integer function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "    df1 = df.copy()\n",
    "    \n",
    "    for curr_column in split_dict.keys():\n",
    "        \n",
    "        \n",
    "        df1[curr_column] = df1[curr_column].str.split(split_dict[curr_column]).str[0].astype(float)\n",
    "\n",
    "    \n",
    "    return df1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e32c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string_to_1integer(df, split_dict):\n",
    "    # split_string_to_1integer function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "    df1 = df.copy()\n",
    "    \n",
    "    def process_value_stromverbrauch(value):\n",
    "    # process_value_stromverbrauch function begins\n",
    "        \n",
    "    # Process done by the function\n",
    "        try:\n",
    "            \n",
    "            if np.isnan(value):\n",
    "                \n",
    "                return 0\n",
    "                     \n",
    "        except:\n",
    "\n",
    "        \n",
    "            try:\n",
    "                \n",
    "                result_value = value.split(split_dict[curr_column])[0]\n",
    "\n",
    "                if result_value.count(\".\") > 1:\n",
    "\n",
    "                    result_value = result_value.split(\".\")[0]\n",
    "\n",
    "                result_value = float(result_value)\n",
    "                \n",
    "                return result_value\n",
    "\n",
    "            except Exception as e:\n",
    "\n",
    "                print(e)\n",
    "                return 0\n",
    "            \n",
    "            \n",
    "    for curr_column in split_dict.keys():\n",
    "        \n",
    "        if curr_column != \"Stromverbrauch\":\n",
    "            df1[curr_column] = df1[curr_column].str.split(split_dict[curr_column]).str[0].astype(float)\n",
    "        \n",
    "        else:\n",
    "            df1[curr_column] = df1[curr_column].apply(process_value_stromverbrauch)\n",
    "\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1809a366",
   "metadata": {},
   "source": [
    "### Strings containing comma-seperated numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143fd3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string_to_1integer_complex(df, split_dict):\n",
    "    # split_string_to_1integer_complex function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "    df1 = df.copy()\n",
    "    \n",
    "    for curr_column in split_dict.keys():\n",
    "        \n",
    "        df1[curr_column] = df1[curr_column].str.split(split_dict[curr_column]).str[0]\n",
    "        df1[curr_column] = df1[curr_column].str.split(\".\").str[0].astype(float)\n",
    "\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86efc03",
   "metadata": {},
   "source": [
    "## Convert Miscellaneous to Floats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13cfc80",
   "metadata": {},
   "source": [
    "### Converting Kraftstoffverbrauch to Floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4ade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_fuel_consumption(df, column_name):\n",
    "    # convert_fuel_consumption function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "    df1 = df.copy()\n",
    "\n",
    "    # Define regular expression patterns to extract komb., innerorts, and außerorts values\n",
    "    komb_pattern = r'(\\d+,\\d+) l/100 km \\(komb.\\)'\n",
    "    innerorts_pattern = r'(\\d+,\\d+) l/100 km \\(innerorts\\)'\n",
    "    außerorts_pattern = r'(\\d+,\\d+) l/100 km \\(außerorts\\)'\n",
    "\n",
    "    # Extract komb., innerorts, and außerorts values using regex\n",
    "    komb_values = df1[column_name].str.extract(komb_pattern)\n",
    "    innerorts_values = df1[column_name].str.extract(innerorts_pattern)\n",
    "    außerorts_values = df1[column_name].str.extract(außerorts_pattern)\n",
    "\n",
    "    # Replace commas with dots and convert to float\n",
    "    komb_values = komb_values.replace(',', '.', regex=True).astype(float)\n",
    "    innerorts_values = innerorts_values.replace(',', '.', regex=True).astype(float)\n",
    "    außerorts_values = außerorts_values.replace(',', '.', regex=True).astype(float)\n",
    "\n",
    "    # Calculate mean for rows containing both innerorts and außerorts values\n",
    "    mean_values = (komb_values + innerorts_values + außerorts_values) / (komb_values.notna() + innerorts_values.notna() + außerorts_values.notna())\n",
    "\n",
    "    # Replace NaNs in mean_values with komb_values\n",
    "    mean_values.fillna(komb_values, inplace=True)\n",
    "\n",
    "    # Replace NaNs in mean_values with innerorts_values\n",
    "    mean_values.fillna(innerorts_values, inplace=True)\n",
    "\n",
    "    # Replace NaNs in mean_values with außerorts_values\n",
    "    mean_values.fillna(außerorts_values, inplace=True)\n",
    "\n",
    "    # Replace NaNs in mean_values with 0\n",
    "    mean_values.fillna(0, inplace=True)\n",
    "\n",
    "    # Assign mean_values to the column in the DataFrame\n",
    "    df1[column_name] = mean_values\n",
    "    \n",
    "    return df1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e26235",
   "metadata": {},
   "source": [
    "### Converting Barzahlungspreis to Floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb05b246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_barzahlungspreis_to_float(df, column_name):\n",
    "    # convert_barzahlungspreis_to_float function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "    df1 = df.copy()\n",
    "    \n",
    "    # Replace missing values with NaN\n",
    "    df1[column_name] = df1[column_name].fillna(float('nan'))\n",
    "    \n",
    "    # Remove unwanted characters from the specified column\n",
    "    df1[column_name] = df1[column_name].str.replace('€', '', regex=True).str.replace('.', '', regex=True).str.replace(',', '.', regex=True).str.replace('-', '', regex=True).str.strip()\n",
    "    \n",
    "    # Convert to float, handling any potential errors\n",
    "    try:\n",
    "        df1[column_name] = df1[column_name].astype(float)\n",
    "    except ValueError:\n",
    "        # If conversion fails, try a different approach\n",
    "        df1[column_name] = df1[column_name].apply(lambda x: ''.join(filter(str.isdigit, str(x))))\n",
    "        df1[column_name] = df1[column_name].replace('', '0').astype(float)\n",
    "\n",
    "    # Convert to integer if it's an integer value\n",
    "    if df1[column_name].dtype == 'float64':\n",
    "        df1[column_name] = df1[column_name].apply(lambda x: int(x) if x.is_integer() else x)\n",
    "        \n",
    "    df1[column_name] = df1[column_name].replace(0, np.nan, regex = False)\n",
    "    \n",
    "    return df1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8564ca86",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1179a8",
   "metadata": {},
   "source": [
    "## Missing Values to Default Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d676823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_to_default(df, replacement_dict):\n",
    "    # missing_values_to_default function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "    df1 = df.copy()\n",
    "    \n",
    "    for curr_column in replacement_dict.keys():\n",
    "        \n",
    "        df1[curr_column] = df1[curr_column].fillna(replacement_dict[curr_column][0])\n",
    "        \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8c63d1",
   "metadata": {},
   "source": [
    "## Missing values in specific columns to mean values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d6281d",
   "metadata": {},
   "source": [
    "### Mean Value for the Age of the car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd0800d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_median_value_based_on_erstzulassung(df, column_to_mean, grouping_variable):\n",
    "    # get_median_value_based_on_erstzulassung function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "    df1 = df.copy()\n",
    "    \n",
    "    mean_values = df1.groupby(df1[grouping_variable].astype(str).str[-4:])[column_to_mean].mean().round()\n",
    "    year_means = dict(mean_values)\n",
    "    \n",
    "    df1[column_to_mean] = df1.apply(lambda row: year_means.get(str(row[grouping_variable])[-4:], row[column_to_mean]), axis=1)\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc43386",
   "metadata": {},
   "source": [
    "### Mean Values based on model and brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e243d1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_with_mode_by_2groups(df, replacement_columns, group_column1, group_column2):\n",
    "    # fill_missing_with_mode_by_2groups function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "    df1 = df.copy()\n",
    "    \n",
    "    for replacement_column in replacement_columns:\n",
    "\n",
    "        # Calculate the mode of each group\n",
    "        mode_per_group = df1.groupby([group_column1, group_column2])[replacement_column].apply(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
    "\n",
    "        # Fill missing values using mode of corresponding group\n",
    "        df1[replacement_column] = df1.apply(lambda row: mode_per_group.get((row[group_column1], row[group_column2]), np.nan) if pd.isnull(row[replacement_column]) else row[replacement_column], axis=1)\n",
    "\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d14cb0c",
   "metadata": {},
   "source": [
    "### Mean Values based on model, brand and Leistung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8db823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_with_mode_by_3groups(df, replacement_columns, group_column1, group_column2, group_column3):\n",
    "    # fill_missing_with_mode_by_3groups function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "    df1 = df.copy()\n",
    "    \n",
    "    for replacement_column in replacement_columns:\n",
    "        # Calculate the mode of each group\n",
    "        mode_per_group = df1.groupby([group_column1, group_column2, group_column3])[replacement_column].apply(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
    "\n",
    "        # Fill missing values using mode of corresponding group\n",
    "        df1[replacement_column] = df1.apply(lambda row: mode_per_group.get((row[group_column1], row[group_column2], row[group_column3]), np.nan) if pd.isnull(row[replacement_column]) else row[replacement_column], axis=1)\n",
    "\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360918b0",
   "metadata": {},
   "source": [
    "# Split Multi Feature Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd85ad8d",
   "metadata": {},
   "source": [
    "## And transform to count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d5f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_strings_in_column(df, column_names):\n",
    "    # count_strings_in_column function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "    # Define a regular expression pattern to identify the split points  \n",
    "    pattern = r'(?<=[a-z])(?=[A-Z])'\n",
    "    \n",
    "    df1 = df.copy()\n",
    "    \n",
    "    for column_name in column_names:\n",
    "        \n",
    "        df2 = df1.copy()\n",
    "        print(\"Current Feature:\", column_name)\n",
    "\n",
    "        # Split the string in the specified column based on the pattern\n",
    "        split_values = df2[column_name].str.split(pattern, expand=True)\n",
    "\n",
    "        # Rename the columns\n",
    "        split_values.columns = [f\"{column_name}_{i}\" for i in range(1, split_values.shape[1] + 1)]\n",
    "\n",
    "        # Concatenate the original DataFrame with the split values DataFrame\n",
    "        df3 = pd.concat([df2, split_values], axis=1)\n",
    "\n",
    "\n",
    "        counts = []\n",
    "        \n",
    "        for index, row in df3[[col for col in df3.columns if column_name in col]].iterrows():\n",
    "            # Count the number of non-\"None\" values in the row\n",
    "            count = sum(1 for value in row if value != \"None\" and not pd.isnull(value))\n",
    "            counts.append(count)\n",
    "\n",
    "        df1[column_name] = counts\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75291286",
   "metadata": {},
   "source": [
    "## And Create Extra Columns for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67b36c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_columns(df, column_names):\n",
    "    # create_feature_columns function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "    # Define a regular expression pattern to identify the split points  \n",
    "    pattern = r'(?<=[a-z])(?=[A-Z])'\n",
    "    \n",
    "    df1 = df.copy()\n",
    "    \n",
    "    for column_name in column_names:\n",
    "        \n",
    "        df2 = df1.copy()\n",
    "        print(\"Current Feature:\", column_name)\n",
    "\n",
    "        # Split the string in the specified column based on the pattern\n",
    "        split_values = df2[column_name].dropna().str.split(pattern, expand=True)\n",
    "\n",
    "        # Flatten the split values into a single list of features\n",
    "        all_features = split_values.values.flatten()\n",
    "        \n",
    "        # Remove None and NaN values and get unique features\n",
    "        all_features = [feature for feature in all_features if feature not in [\"None\", None] and pd.notna(feature)]\n",
    "        unique_features = set(all_features)\n",
    "\n",
    "        # Create a column for each unique feature\n",
    "        for feature in unique_features:\n",
    "            df1[f\"{column_name}_{feature}\"] = df2[column_name].apply(lambda x: 1 if isinstance(x, str) and feature in x else 0)\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    return df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "202f5cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_columns(df, column_names):\n",
    "    # create_feature_columns function begins\n",
    "    # Define a regular expression pattern to identify the split points  \n",
    "    pattern = r'(?<=[a-z])(?=[A-Z])'\n",
    "    \n",
    "    # Process done by the function\n",
    "    df1 = df.copy()\n",
    "    \n",
    "    for column_name in column_names:\n",
    "        \n",
    "        df2 = df1.copy()\n",
    "        print(\"Current Feature:\", column_name)\n",
    "\n",
    "        # Split the string in the specified column based on the pattern\n",
    "        split_values = df2[column_name].dropna().str.split(pattern, expand=True)\n",
    "\n",
    "        # Flatten the split values into a single list of features\n",
    "        all_features = split_values.values.flatten()\n",
    "        \n",
    "        # Remove None and NaN values and get unique features\n",
    "        all_features = [feature for feature in all_features if feature not in [\"None\", None] and pd.notna(feature)]\n",
    "        unique_features = set(all_features)\n",
    "\n",
    "        # Initialize a dictionary to hold the new columns\n",
    "        feature_columns = {}\n",
    "\n",
    "        # Create a column for each unique feature\n",
    "        for feature in unique_features:\n",
    "            feature_columns[f\"{column_name}_{feature}\"] = df2[column_name].apply(lambda x: 1 if isinstance(x, str) and feature in x else 0)\n",
    "        \n",
    "        # Concatenate the new feature columns to the original DataFrame\n",
    "        df1 = pd.concat([df1, pd.DataFrame(feature_columns)], axis=1)\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    return df1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb83bf60",
   "metadata": {},
   "source": [
    "# Date Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22920d8c",
   "metadata": {},
   "source": [
    "## Scraped \"date\" and \"time\" to timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cc9cfc",
   "metadata": {},
   "source": [
    "### Create New Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d6dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_timestamp(df, date_column, time_column):\n",
    "    # convert_to_timestamp function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "    df1 = df.copy()\n",
    "    \n",
    "    df1[\"date_scraped\"] = pd.to_datetime(df1[date_column] + ' ' + df1[time_column], format='%Y-%m-%d %H-%M-%S')\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5835ac5",
   "metadata": {},
   "source": [
    "## Erstzulassung to Age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f91047c",
   "metadata": {},
   "source": [
    "### Split Erstzulassung to \"Erstzulassung_Jahr\" and \"Erstzulassung_Monat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91e400b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_month_year(df, column_name):\n",
    "    # split_month_year function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "    df1 = df.copy()\n",
    "    \n",
    "    # Split the date column into separate month and year columns\n",
    "    df1[['Erstzulassung_Monat', 'Erstzulassung_Jahr']] = df1[column_name].str.split('/', expand=True)\n",
    "    \n",
    "    # Convert month and year columns to integers\n",
    "    df1['Erstzulassung_Monat'] = df1['Erstzulassung_Monat'].astype(float).fillna(0).astype(int)\n",
    "    df1['Erstzulassung_Jahr'] = df1['Erstzulassung_Jahr'].astype(float).fillna(0).astype(int)\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f5c3c2",
   "metadata": {},
   "source": [
    "### Create Age Variable from that column (and delete Zulassungsmonat/Jahr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2604a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming features to age:\n",
    "def convert_zulassung_to_age(df, year_var, month_var):\n",
    "    # convert_zulassung_to_age function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "    df1 = df.copy()\n",
    "    \n",
    "    age_list = []\n",
    "    for i in range(0, len(df1[year_var])):\n",
    "        \n",
    "        scraping_time = df1[\"date_scraped\"].iloc[i]\n",
    "        # calculate difference in years since registration\n",
    "        curr_years = scraping_time.year - df1[year_var].iloc[i]\n",
    "\n",
    "\n",
    "        # calculate difference in months between this year and registration year\n",
    "        curr_month_difference = scraping_time.month - df1[month_var].iloc[i]\n",
    "\n",
    "        # add up variables to get age of the car\n",
    "        curr_age = curr_years + curr_month_difference/12\n",
    "        age_list.append(curr_age)\n",
    "        \n",
    "    df1[\"Alter\"] = age_list\n",
    "    df1[\"Alter\"] = df1[\"Alter\"].astype(\"float\")\n",
    "    df1[\"Alter\"] = np.round(df1[\"Alter\"], 2)\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6d0fb7",
   "metadata": {},
   "source": [
    "# Drop Observations that contain NaNs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b67287",
   "metadata": {},
   "source": [
    "# Outlier Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69a8567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, group_cols, target_col, lower_quantile=0.05, upper_quantile=0.95):\n",
    "    # remove_outliers function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "\n",
    "    # work on copy\n",
    "    df1 = df.copy()\n",
    "\n",
    "    # Group by the specified columns\n",
    "    grouped = df1.groupby(group_cols)\n",
    "    \n",
    "    # Create a mask to filter out the outliers\n",
    "    mask = pd.Series([True] * len(df1), index=df1.index)\n",
    "    \n",
    "    # Iterate through each group\n",
    "    for name, group in grouped:\n",
    "        # Compute the lower and upper percentile values for the target column\n",
    "        lower_threshold = group[target_col].quantile(lower_quantile)\n",
    "        upper_threshold = group[target_col].quantile(upper_quantile)\n",
    "        \n",
    "        # Mark rows that are within the bounds\n",
    "        mask.loc[group.index] = (group[target_col] >= lower_threshold) & (group[target_col] <= upper_threshold)\n",
    "    \n",
    "    # Return the DataFrame with outliers removed\n",
    "    return df1[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f6b03",
   "metadata": {},
   "source": [
    "# Rename Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc40e9c",
   "metadata": {},
   "source": [
    "# Rearrange Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf3665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_columns(df, first_columns):\n",
    "    # rearrange_columns function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "    df1 = df.copy()\n",
    "    # Select the specified first columns\n",
    "    first_columns_df = df1[first_columns]\n",
    "    \n",
    "    # Select the remaining columns\n",
    "    remaining_columns = df1.drop(columns = first_columns)\n",
    "    \n",
    "    # Concatenate the specified first columns with the remaining columns\n",
    "    rearranged_df = pd.concat([first_columns_df, remaining_columns], axis=1)\n",
    "    \n",
    "    return rearranged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e546db45",
   "metadata": {},
   "source": [
    "# Data Cleaning Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e7973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning_overview(initial_data, after_data):\n",
    "    # data_cleaning_overview function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "    # calculate column metrics\n",
    "    n_initial_columns = len(initial_data.columns)\n",
    "    n_after_columns = len(after_data.columns)\n",
    "    n_dropped_columns = n_initial_columns - n_after_columns\n",
    "    perc_dropped_columns = np.round(n_dropped_columns/n_initial_columns, 2)*100\n",
    "    \n",
    "    # calculate observation metrics\n",
    "    n_initial_obs = len(initial_data)\n",
    "    n_after_obs = len(after_data)\n",
    "    n_dropped_obs = n_initial_obs - n_after_obs\n",
    "    perc_dropped_obs = np.round(n_dropped_obs/n_initial_obs, 2)*100\n",
    "    \n",
    "    \n",
    "    print(\"################## Data Cleaning Overview ##################\")\n",
    "    print(\"\")\n",
    "    print(f\"Initial Columns: {n_initial_columns}\")\n",
    "    print(f\"Final Columns for model training: {n_after_columns}\")\n",
    "    print(f\"Dropped {n_dropped_columns}({perc_dropped_columns}%) of all columns!\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(f\"Initial Observations: {n_initial_obs}\")\n",
    "    print(f\"Final Observations for model training: {n_after_obs}\")\n",
    "    print(f\"Dropped {n_dropped_obs}({perc_dropped_obs}%) of all observations!\")\n",
    "    print(\"\")\n",
    "    print(\"################## /Data Cleaning Overview ##################\")\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed7d162",
   "metadata": {},
   "source": [
    "# Data Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e82eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clean_data(df, output_path):\n",
    "    # save_clean_data function begins\n",
    "    # Get the current datetime\n",
    "    now = datetime.now()\n",
    "    \n",
    "    # Process done by the function\n",
    "    # Format the datetime into the desired string format\n",
    "    formatted_time = now.strftime(\"%Y_%m_%d %H-%M\")\n",
    "    output_file = f\"{output_path}\\cleaned_data_{formatted_time}.csv\"\n",
    " \n",
    "    df.to_csv(output_file, index = False)\n",
    "    \n",
    "    print(f\"Succesfully saved cleaned dataframe to: \\n\\n{output_file} !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "821.111px",
    "left": "83px",
    "top": "111.441px",
    "width": "416.441px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
