{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42bfc44a",
   "metadata": {},
   "source": [
    "# Get Best Offers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c9ce7f",
   "metadata": {},
   "source": [
    "## Make Predictions on a Subselection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69aa5591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_subselection_xgb(df, manufacturer, car_model, max_mileage, initial_approval, n_days, model):\n",
    "    # predict_subselection_xgb function begins\n",
    "        \n",
    "    # Process done by the function\n",
    "    manufactuer_formatted = str(\"Manufacturer_\" + str(manufacturer))\n",
    "    car_model_formatted = str(\"Model_\" + str(car_model))\n",
    "    \n",
    "    ### 1) subselect the specified car model as well as max_kilometers and initial_approval ###\n",
    "    car_model_subset = df.copy()\n",
    "    car_model_subset = car_model_subset[(car_model_subset[manufactuer_formatted] == 1) & (car_model_subset[car_model_formatted] == 1) & (car_model_subset[\"Erstzulassung_Jahr\"] >= initial_approval) & (car_model_subset[\"Mileage\"] <= max_mileage)]\n",
    "    \n",
    "    ### 2) convert date_scraped to timestamp object ###\n",
    "    car_model_subset_days = car_model_subset.copy()\n",
    "    car_model_subset_days.loc[:, 'Date_scraped'] = pd.to_datetime(car_model_subset.loc[:, 'Date_scraped'])\n",
    "    \n",
    "    ### 3) only respect data scraped in the last n days ###\n",
    "    \n",
    "    # get current date\n",
    "    current_date = datetime.now()\n",
    "    \n",
    "    # get date n_days back\n",
    "    date_n_days_ago = current_date - timedelta(days= n_days)\n",
    "    \n",
    "    # subselect only observations scraped since date_n_days_ago\n",
    "    n_days_subset = car_model_subset_days[car_model_subset_days[\"Date_scraped\"] >= date_n_days_ago]\n",
    "    \n",
    "    if len(car_model_subset_days) < 1:\n",
    "        \n",
    "        return \"no data\"\n",
    "    \n",
    "    ### 4) Make Predictions ###\n",
    "    \n",
    "    print(\"Making Price Predictions...\")\n",
    "\n",
    "    prediction_subset = n_days_subset.copy()\n",
    "    \n",
    "    # extract true prices and URL\n",
    "    true_prices = prediction_subset[\"Price\"]\n",
    "    model_urls = prediction_subset[\"URL\"]\n",
    "    \n",
    "    \n",
    "    # drop true prices and URL for model predictions\n",
    "    prediction_subset.drop(columns = [\"Price\"], inplace = True)\n",
    "    prediction_subset.drop(columns = [\"URL\"], inplace = True)\n",
    "    prediction_subset.drop(columns = [\"Date_scraped\"], inplace = True)\n",
    "    \n",
    "    # predict prices for observations\n",
    "    predicted_y = model.predict(prediction_subset)\n",
    "    \n",
    "    # merge the data frame back together and calculate a price diff\n",
    "    merged_df = prediction_subset.copy()\n",
    "    merged_df[\"Price\"] = true_prices\n",
    "    merged_df[\"Predicted Price\"] = predicted_y\n",
    "    merged_df[\"Price Diff\"] = merged_df[\"Predicted Price\"] - merged_df[\"Price\"]\n",
    "    merged_df[\"URL\"] = model_urls\n",
    "    \n",
    "    merged_df[\"Manufacturer\"] = manufacturer\n",
    "    merged_df[\"Model\"] = car_model\n",
    "    \n",
    "    # sort the dataframe such that good offers (Predicted Price >> Price) are shown at top\n",
    "    sorted_prediction_df = merged_df.sort_values([\"Price Diff\"], ascending = False)\n",
    "    \n",
    "    print(f\"Predictions done! Found {len(sorted_prediction_df)} observations for {manufacturer} {car_model} and max mileage of {max_mileage}km and Initial Approval >= {initial_approval}!\")\n",
    "    \n",
    "    return sorted_prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073b02eb",
   "metadata": {},
   "source": [
    "## Check Best Offers on Availability and Create Result DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d635ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_offers_efficiently_xgb(df, n_best_offers, area_input, change_vpn, sleep_time):\n",
    "    # get_best_offers_efficiently_xgb function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "\n",
    "    \n",
    "    sorted_prediction_df = df.copy()\n",
    "    \n",
    "    # create empty df for results\n",
    "    result_df = pd.DataFrame()\n",
    "    \n",
    "    if change_vpn:\n",
    "        \n",
    "        instructions_vpn = initialize_VPN(area_input = area_input)\n",
    "        rotate_VPN(instructions_vpn)\n",
    "    \n",
    "    print(\"Checking Availability and Creating Result DataFrame!\")\n",
    "    \n",
    "    # start an index for the while loop\n",
    "    index = 0\n",
    "    \n",
    "    # loop over sorted prediction df\n",
    "    while len(result_df) < n_best_offers and index <= len(sorted_prediction_df)+1:\n",
    "        \n",
    "        \n",
    "        # extract current entry\n",
    "        curr_entry = sorted_prediction_df[index : index+1].copy()\n",
    "        \n",
    "        # extract current URL\n",
    "        \n",
    "        curr_url = curr_entry[\"URL\"][curr_entry.index[0]]\n",
    "\n",
    "        try:\n",
    "\n",
    "            response = requests.get(curr_url, timeout = 10)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                \n",
    "                try:\n",
    "                    # get the image_url\n",
    "                    html = response.text\n",
    "                    doc = BeautifulSoup(html, \"html.parser\")\n",
    "                    img_url = doc.find('picture', class_ ='ImageWithBadge_picture__XJG24').find()\n",
    "                    img_url = \"/\".join(str(img_url).split()[3].split(\"=\")[1][1:-1].split(\"/\")[:-1])\n",
    "\n",
    "                    # save successful observation to result dataframe\n",
    "                    curr_entry[\"Image URL\"] = img_url\n",
    "                    result_df = pd.concat([result_df, curr_entry])\n",
    "                    \n",
    "                    #print(f\"Succesfully created entry at index {index}!\")\n",
    "\n",
    "                    # let the scraper sleep to remain undetected\n",
    "                    time.sleep(sleep_time)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                            \n",
    "                    # let the scraper sleep to remain undetected\n",
    "                    #print(f\"Failed to create entry at index {index}!\")\n",
    "                    #print(f\"Error: {e}\")\n",
    "                    \n",
    "                    time.sleep(sleep_time)\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "        # handle and log the exception\n",
    "        except requests.RequestException as e:\n",
    "            \n",
    "            # let the scraper sleep to remain undetected\n",
    "            time.sleep(sleep_time)\n",
    "            \n",
    "\n",
    "        # increase index by 1 for each iteration\n",
    "        index += 1\n",
    "        \n",
    "    if len(result_df) < 1:\n",
    "        \n",
    "        print(\"Found no available Offers! [2nd function]\")\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print(f\"Best Offers DataFrame created with {len(result_df)} Offers!\")\n",
    "        \n",
    "    ## Formating Results\n",
    "    result_df.rename(columns = {\"Predicted Price\" : \"Fair Price\", \"Price\" : \"Offer Price\", \"Price Diff\" : \"Savings\"}, inplace = True)\n",
    "    \n",
    "    # round results\n",
    "    result_df_rounded = result_df.round(2)\n",
    "    \n",
    "    \n",
    "    # add € to Price columns\n",
    "    price_columns = [\"Fair Price\", \"Offer Price\", \"Savings\"]\n",
    "\n",
    "    # Adding the € sign to the specified columns\n",
    "    for col in price_columns:\n",
    "\n",
    "        result_df_rounded[col] = result_df_rounded[col].apply(lambda x: f\"{x:.2f}€\")\n",
    "\n",
    "\n",
    "    return result_df_rounded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146322ab",
   "metadata": {},
   "source": [
    "## All in One Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa3403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_offers_in_one_efficiently_xgb(df, manufacturer, car_model, max_mileage, initial_approval, n_days, area_input,\n",
    "    # get_best_offers_in_one_efficiently_xgb function begins\n",
    "                                       sleep_time, change_vpn, model, n_best_offers):\n",
    "    \n",
    "    # Process done by the function\n",
    "    # get available subset\n",
    "    sorted_prediction_df = predict_subselection_xgb(df, manufacturer, car_model, max_mileage, initial_approval, n_days, model)\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    if not isinstance(sorted_prediction_df, pd.DataFrame):\n",
    "        \n",
    "        print(\"No Data found [1st Function]!\")\n",
    "\n",
    "        return \n",
    "\n",
    "    best_offers_df = get_best_offers_efficiently_xgb(sorted_prediction_df, n_best_offers, area_input, change_vpn, sleep_time)\n",
    "        \n",
    "        \n",
    "    return best_offers_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1727d8",
   "metadata": {},
   "source": [
    "# Improved Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806ddf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def predict_subselection_xgb(df, manufacturer, car_model, max_mileage, initial_approval, n_days, model):\n",
    "    # predict_subselection_xgb function begins\n",
    "    manufactuer_formatted = str(\"Manufacturer_\" + str(manufacturer))\n",
    "    car_model_formatted = str(\"Model_\" + str(car_model))\n",
    "    \n",
    "    # Process done by the function\n",
    "    ### 1) subselect the specified car model as well as max_kilometers and initial_approval ###\n",
    "    car_model_subset = df.copy()\n",
    "    car_model_subset = car_model_subset[\n",
    "        (car_model_subset[manufactuer_formatted] == 1) &\n",
    "        (car_model_subset[car_model_formatted] == 1) &\n",
    "        (car_model_subset[\"Erstzulassung_Jahr\"] >= initial_approval) &\n",
    "        (car_model_subset[\"Mileage\"] <= max_mileage)\n",
    "    ]\n",
    "    \n",
    "    if car_model_subset.empty:\n",
    "        print(\"No data found after subselecting car model.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    ### 2) convert date_scraped to timestamp object ###\n",
    "    car_model_subset_days = car_model_subset.copy()\n",
    "    car_model_subset_days.loc[:, 'Date_scraped'] = pd.to_datetime(car_model_subset.loc[:, 'Date_scraped'])\n",
    "    \n",
    "    ### 3) only respect data scraped in the last n days ###\n",
    "    \n",
    "    # get current date\n",
    "    current_date = datetime.now()\n",
    "    \n",
    "    # get date n_days back\n",
    "    date_n_days_ago = current_date - timedelta(days=n_days)\n",
    "    \n",
    "    # subselect only observations scraped since date_n_days_ago\n",
    "    n_days_subset = car_model_subset_days[car_model_subset_days[\"Date_scraped\"] >= date_n_days_ago]\n",
    "    \n",
    "    if n_days_subset.empty:\n",
    "        print(\"No data found in the last n days.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    ### 4) Make Predictions ###\n",
    "    \n",
    "    print(\"Making Price Predictions...\")\n",
    "    \n",
    "    prediction_subset = n_days_subset.copy()\n",
    "    \n",
    "    # extract true prices and URL\n",
    "    true_prices = prediction_subset[\"Price\"]\n",
    "    model_urls = prediction_subset[\"URL\"]\n",
    "    \n",
    "    # drop true prices and URL for model predictions\n",
    "    prediction_subset.drop(columns=[\"Price\", \"URL\", \"Date_scraped\"], inplace=True)\n",
    "    \n",
    "    # predict prices for observations\n",
    "    predicted_y = model.predict(prediction_subset)\n",
    "    \n",
    "    # merge the data frame back together and calculate a price diff\n",
    "    merged_df = prediction_subset.copy()\n",
    "    merged_df[\"Price\"] = true_prices\n",
    "    merged_df[\"Predicted Price\"] = predicted_y\n",
    "    merged_df[\"Price Diff\"] = merged_df[\"Predicted Price\"] - merged_df[\"Price\"]\n",
    "    merged_df[\"URL\"] = model_urls\n",
    "    merged_df[\"Manufacturer\"] = manufacturer\n",
    "    merged_df[\"Model\"] = car_model\n",
    "    \n",
    "    # sort the dataframe such that good offers (Predicted Price >> Price) are shown at top\n",
    "    sorted_prediction_df = merged_df.sort_values([\"Price Diff\"], ascending=False)\n",
    "    \n",
    "    print(f\"Predictions done! Found {len(sorted_prediction_df)} observations for {manufacturer} {car_model} and max mileage of {max_mileage}km and Initial Approval >= {initial_approval}!\")\n",
    "    \n",
    "    return sorted_prediction_df\n",
    "\n",
    "def get_best_offers_efficiently_xgb(df, n_best_offers, sleep_time):\n",
    "    # get_best_offers_efficiently_xgb function begins\n",
    "    \n",
    "    # Process done by the function\n",
    "    if df.empty:\n",
    "        print(\"Input dataframe is empty. No offers to check.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    sorted_prediction_df = df.copy()\n",
    "    \n",
    "    # create empty df for results\n",
    "    result_df = pd.DataFrame()\n",
    "   \n",
    "    print(\"Checking Availability and Creating Result DataFrame!\")\n",
    "    \n",
    "    # start an index for the while loop\n",
    "    index = 0\n",
    "    \n",
    "    # loop over sorted prediction df\n",
    "    while len(result_df) < n_best_offers and index <= len(sorted_prediction_df):\n",
    "        # extract current entry\n",
    "        curr_entry = sorted_prediction_df[index:index+1].copy()\n",
    "        \n",
    "        # Check if curr_entry is empty\n",
    "        if curr_entry.empty:\n",
    "            index += 1\n",
    "            continue\n",
    "        \n",
    "        # extract current URL\n",
    "        curr_url = curr_entry[\"URL\"].iloc[0]\n",
    "\n",
    "        try:\n",
    "            response = requests.get(curr_url, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                try:\n",
    "                    # get the image_url\n",
    "                    html = response.text\n",
    "                    doc = BeautifulSoup(html, \"html.parser\")\n",
    "                    img_url = doc.find('picture', class_='ImageWithBadge_picture__XJG24').find()\n",
    "                    img_url = \"/\".join(str(img_url).split()[3].split(\"=\")[1][1:-1].split(\"/\")[:-1])\n",
    "\n",
    "                    # save successful observation to result dataframe\n",
    "                    curr_entry[\"Image URL\"] = img_url\n",
    "                    result_df = pd.concat([result_df, curr_entry])\n",
    "                    \n",
    "                    # let the scraper sleep to remain undetected\n",
    "                    time.sleep(sleep_time)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # let the scraper sleep to remain undetected\n",
    "                    time.sleep(sleep_time)\n",
    "        except requests.RequestException as e:\n",
    "            # let the scraper sleep to remain undetected\n",
    "            time.sleep(sleep_time)\n",
    "        \n",
    "        # increase index by 1 for each iteration\n",
    "        index += 1\n",
    "        \n",
    "    if result_df.empty:\n",
    "        print(\"Found no available Offers! [2nd function]\")\n",
    "    else:\n",
    "        print(f\"Best Offers DataFrame created with {len(result_df)} Offers!\")\n",
    "        \n",
    "    ## Formating Results\n",
    "    result_df.rename(columns={\"Predicted Price\": \"Fair Price\", \"Price\": \"Offer Price\", \"Price Diff\": \"Savings\"}, inplace=True)\n",
    "    \n",
    "    # round results\n",
    "    result_df_rounded = result_df.round(2)\n",
    "    \n",
    "    # add € to Price columns\n",
    "    price_columns = [\"Fair Price\", \"Offer Price\", \"Savings\"]\n",
    "\n",
    "    # Adding the € sign to the specified columns\n",
    "    for col in price_columns:\n",
    "        result_df_rounded[col] = result_df_rounded[col].apply(lambda x: f\"{x:.2f}€\")\n",
    "\n",
    "    return result_df_rounded\n",
    "\n",
    "def get_best_offers_in_one_efficiently_xgb(df, manufacturer, car_model, max_mileage, initial_approval, n_days,\n",
    "    # get_best_offers_in_one_efficiently_xgb function begins\n",
    "                                           sleep_time, model, n_best_offers):\n",
    "    # get available subset\n",
    "    sorted_prediction_df = predict_subselection_xgb(df, manufacturer, car_model, max_mileage, initial_approval, n_days, model)\n",
    "    \n",
    "    # Process done by the function\n",
    "    if sorted_prediction_df.empty:\n",
    "        print(\"No Data found [1st Function]!\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    best_offers_df = get_best_offers_efficiently_xgb(sorted_prediction_df, n_best_offers, sleep_time)\n",
    "    \n",
    "    best_offers_df = best_offers_df.reset_index(drop = True)\n",
    "    \n",
    "    return best_offers_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
