{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06381004",
   "metadata": {},
   "source": [
    "# Pipeline Functions\n",
    "\n",
    "In this script we are defining the functions that are doing the actual data retrieving part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2d2757",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "Import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1547b33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from nordvpn_switcher import initialize_VPN, rotate_VPN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3d48b6",
   "metadata": {},
   "source": [
    "# Webscraping Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875cce97",
   "metadata": {},
   "source": [
    "## Paramater Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee607c71",
   "metadata": {},
   "source": [
    "Function to scrape data from a website in a loop with error handling and logging.\n",
    "\n",
    "Parameters:\n",
    "- `user`: str, the username or identifier of the user performing the scraping.\n",
    "- `sleep_interval`: tuple, interval for sleeping between requests to avoid being blocked by the website.\n",
    "- `attribute_exception_list`: list, a list of attributes to exclude when scraping car information.\n",
    "- `instructions_vpn`: str, instructions for rotating the VPN connection.\n",
    "- `n_tries`: int, number of attempts to scrape data.\n",
    "- `max_pages`: int, maximum number of pages to scrape for each car model.\n",
    "\n",
    "<br>\n",
    "Returns:\n",
    "\n",
    "- `result_df`: pandas DataFrame, scraped data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f59af4",
   "metadata": {},
   "source": [
    "## Base Function\n",
    "\n",
    "This is the first iteration of the data-retrieving loop. More information on how the loop operates can be found in the comments inside the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75cb32f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform web scraping with optimization\n",
    "def webscrape_loop(user, sleep_interval, attribute_exception_list,\n",
    "                             instructions_vpn, in_out_path,\n",
    "                             n_tries = 10, max_pages = 20, do_backup = False,\n",
    "                             adage = 7, use_recency = False):\n",
    "    \n",
    "    try:\n",
    "\n",
    "        for _ in range(n_tries):\n",
    "\n",
    "            offer_list = []\n",
    "            try:\n",
    "                # Setting up file paths and initializing variables\n",
    "                input_path = in_out_path\n",
    "                try:\n",
    "                    result_df = pd.read_csv(input_path, low_memory=False)\n",
    "                    all_used_urls = set(result_df[\"url\"])\n",
    "                    print(\"Loaded\", len(result_df), \"entries.\")\n",
    "                    print(\"\")\n",
    "                except FileNotFoundError:\n",
    "                    result_df = pd.DataFrame()\n",
    "                    all_used_urls = set()\n",
    "\n",
    "                n_duplicates = 0\n",
    "                logging_df = pd.read_csv(f\"logging/logging_data/logging_df_{user}.csv\")\n",
    "                logging_df_short = logging_df[logging_df[\"user\"] == user].copy()\n",
    "\n",
    "                # Looping through the logging dataframe\n",
    "                for _, row in logging_df_short.iterrows():\n",
    "                    curr_brand = str(row[\"brand\"])\n",
    "                    curr_model = str(row[\"model\"])\n",
    "\n",
    "                    # Checking year condition for VPN rotation\n",
    "                    if int(row[\"curr_year\"]) < 2025:\n",
    "                        rotate_VPN(instructions_vpn)\n",
    "\n",
    "                        # Iterating over years and pages\n",
    "                        for curr_year in range(int(row[\"curr_year\"]), int(row[\"end_year\"]) + 1):\n",
    "                             \n",
    "                            n_duplicates = 0 # reset number of duplicates for the upcoming scraped year\n",
    "\n",
    "                            for curr_page in range(1, max_pages + 1):\n",
    "\n",
    "                                print(f\"Current Brand: {curr_brand}\\nCurrent Model: {curr_model}\\nCurr Year: {curr_year}\\nCurr Page: {curr_page}\")\n",
    "\n",
    "                                current_datetime = datetime.now()\n",
    "                                formatted_date_hour = current_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                                print(\"Current Time:\", formatted_date_hour)\n",
    "                                # Retrieving page HTML and parsing\n",
    "                                print(f\"Reading Page {curr_page}.\")\n",
    "                                \n",
    "                                if use_recency:\n",
    "                                    model_url = f\"https://www.autoscout24.de/lst/{curr_brand}/{curr_model}/re_{curr_year}?adage={adage}&atype=C&cy=D&damaged_listing=exclude&desc=0&ocs_listing=include&page={curr_page}&powertype=kw&search_id=ehnjs4dnm6&sort=standard&source=listpage_pagination\"\n",
    "\n",
    "                                else:\n",
    "                                    model_url = f\"https://www.autoscout24.de/lst/{curr_brand}/{curr_model}/re_{curr_year}?atype=C&cy=D&damaged_listing=exclude&desc=0&ocs_listing=include&page={curr_page}&powertype=kw&search_id=ehnjs4dnm6&sort=standard&source=listpage_pagination\"\n",
    "\n",
    "                                response = requests.get(model_url)\n",
    "                                html = response.text\n",
    "                                doc = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "                                # Extracting number of offers\n",
    "                                no_of_offers = int(doc.find('div', class_='ListHeader_title_with_sort__Pf4Zw').find('span').text.strip().split()[0].replace(\".\", \"\"))\n",
    "                                print(\"Number of total offers:\", no_of_offers)\n",
    "                                \n",
    "                            \n",
    "                                \n",
    "                                # Handling no offers case\n",
    "                                if no_of_offers == 0:\n",
    "                                    sleep_time = random.uniform(*sleep_interval)\n",
    "                                    time.sleep(sleep_time)\n",
    "                                    print(\"No offers for this year!\")\n",
    "\n",
    "                                    break\n",
    "\n",
    "                                sleep_time = random.uniform(*sleep_interval)\n",
    "                                time.sleep(sleep_time)\n",
    "\n",
    "                                # Extracting offer URLs\n",
    "                                offer_list = [paragraph.get(\"href\") for paragraph in doc.find_all(\"a\")\n",
    "                                              if r'/angebote/' in str(paragraph.get(\"href\")) and r'/leasing/' not in str(paragraph.get(\"href\")) and r'/recommendation/' not in str(paragraph.get(\"href\"))]\n",
    "\n",
    "                                print(f\"Accessing a total of {len(offer_list)} offers on this page!\")\n",
    "\n",
    "                                # Handling last page condition\n",
    "                                if len(offer_list) == 0:\n",
    "                                    print(\"Last page reached!\")\n",
    "                                    break\n",
    "                                else:\n",
    "                                    print(\"Estimated total sleeping time until next page:\", len(offer_list) * np.mean(sleep_interval), \"seconds.\")\n",
    "\n",
    "                                # Looping through offer URLs\n",
    "                                \n",
    "                                \n",
    "                                for item in offer_list:\n",
    "                                    try:\n",
    "                                        curr_item_url = \"https://www.autoscout24.de\" + item\n",
    "                                        if curr_item_url not in all_used_urls:\n",
    "                                            current_delay = random.uniform(*sleep_interval)\n",
    "                                            time.sleep(current_delay)\n",
    "\n",
    "                                            response = requests.get(curr_item_url)\n",
    "                                            html = response.text\n",
    "                                            doc = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "                                            # maybe extract the features manually as we get errors for assigning the right values to the right columns\n",
    "                                            curr_car_dict = {}\n",
    "                                            for key, value in zip(doc.find_all(\"dt\"), doc.find_all(\"dd\")):\n",
    "                                                if key.text not in attribute_exception_list:\n",
    "                                                    curr_car_dict[key.text.replace(\"\\n\", \"\")] = value.text.replace(\"\\n\", \"\")\n",
    "\n",
    "                                            curr_car_dict[\"url\"] = curr_item_url\n",
    "                                            curr_car_dict[\"date\"] = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "                                            curr_car_dict[\"time\"] = datetime.now().strftime(\"%H-%M-%S\")\n",
    "                                            curr_car_dict[\"model\"] = doc.find(\"span\", class_=\"StageTitle_model__EbfjC StageTitle_boldClassifiedInfo__sQb0l\").get_text()\n",
    "                                            curr_car_dict[\"brand\"] = doc.find(\"span\", class_=\"StageTitle_boldClassifiedInfo__sQb0l\").get_text()\n",
    "\n",
    "                                            curr_df = pd.DataFrame.from_dict(curr_car_dict, orient=\"index\").T\n",
    "                                            \n",
    "                                            \n",
    "                                            ################## Janik: New Part ########################\n",
    "                                            \n",
    "                                            # Check if the column exists, if not, add it to the beginning\n",
    "                                            if 'Barzahlungspreis' not in curr_df.columns:\n",
    "                                                curr_df.insert(0,\n",
    "                                                                'Barzahlungspreis',\n",
    "                                                                re.split(r'(?<=-)', doc.find('div', class_='PriceInfo_wrapper__hreB_').find('span', class_='PriceInfo_price__XU0aF').text.strip())[0])\n",
    "                                            \n",
    "                                            #############################################################\n",
    "                                            \n",
    "                                            \n",
    "                                            result_df = pd.concat([result_df, curr_df])\n",
    "                                            all_used_urls.add(curr_item_url)\n",
    "                                        else:\n",
    "                                            n_duplicates = n_duplicates + 1\n",
    "                                            print(\"Duplicate found:\", curr_item_url)\n",
    "                                        \n",
    "\n",
    "                                    except Exception as e:\n",
    "                                        print(\"Error occurred in accessing car url:\", str(e))\n",
    "                                        print(\"\")\n",
    "\n",
    "                                        print(\"CURRENTLY SAVING ALL, DONT STOP!\")\n",
    "                                        logging_df.to_csv(f\"logging/logging_data/logging_df_{user}.csv\", index=False)\n",
    "                                        result_df.to_csv(in_out_path, index=False)\n",
    "                                        print(\"Saved Result and Logging DF and rotating VPN!\")\n",
    "                                        rotate_VPN(instructions_vpn)\n",
    "\n",
    "                                        break\n",
    "\n",
    "                                if len(offer_list) <= 5:\n",
    "                                    print(\"Last page reached!\")\n",
    "                                    break\n",
    "\n",
    "                                print(\"\")\n",
    "\n",
    "                            logging_df.loc[(logging_df[\"user\"] == user) &\n",
    "                                           (logging_df[\"brand\"] == curr_brand) &\n",
    "                                           (logging_df[\"model\"] == curr_model), \"curr_year\"] = curr_year + 1\n",
    "\n",
    "                            logging_df.loc[(logging_df[\"user\"] == user) &\n",
    "                                           (logging_df[\"brand\"] == curr_brand) &\n",
    "                                           (logging_df[\"model\"] == curr_model), \"last_scraped\"] = datetime.now().strftime(\"%Y-%m-%d %H %M\")\n",
    "\n",
    "                            print(\"\")\n",
    "                            \n",
    "                            try:\n",
    "                                print(\"CURRENTLY SAVING LOGGING_DF, DONT STOP!\")\n",
    "                                logging_df.to_csv(f\"logging/logging_data/logging_df_{user}.csv\", index=False)\n",
    "                                print(\"SAVING DONE!\")\n",
    "                                \n",
    "                            except KeyboardInterrupt:\n",
    "                                \n",
    "                                print(\"\")\n",
    "                                print(\"SAVING ALL DUE TO KEYBOARD INTERRUPT!\")\n",
    "                                logging_df.to_csv(f\"logging/logging_data/logging_df_{user}.csv\", index=False)\n",
    "                                result_df.to_csv(in_out_path, index=False)\n",
    "                                print(\"SAVING DONE!\")\n",
    "\n",
    "                            print(\"\")\n",
    "                            print(f\"Amount of offers for this year: {no_of_offers}\\nAmount of Duplicatesfor this year: {n_duplicates}\")\n",
    "                            print(\"\")\n",
    "\n",
    "                            if no_of_offers - n_duplicates > 0:\n",
    "                                print(\"\")\n",
    "\n",
    "                                print(\"CURRENTLY SAVING RESULTS, DONT STOP!\")\n",
    "                                result_df.to_csv(in_out_path, index=False)\n",
    "                                print(\"SAVING DONE!\")\n",
    "                        \n",
    "  \n",
    "\n",
    "                            print(\"\")\n",
    "                            print(\"########################################\")\n",
    "                            print(\"\")\n",
    "\n",
    "                            print(\"\")\n",
    "                            print(\"Current amount of entries in the Result Dataframe:\", len(result_df))\n",
    "\n",
    "\n",
    "                        try:\n",
    "\n",
    "                            if do_backup:\n",
    "                                current_datetime = datetime.now()\n",
    "                                formatted_date_hour = current_datetime.strftime(\"%Y-%m-%d %H %M\")\n",
    "                                result_df.to_csv(f\"scraped_data/{user}/backups/{user}_data_{formatted_date_hour}.csv\")\n",
    "                                print(f\"Saved Dataframe successfully after finishing model {curr_model}!\")\n",
    "\n",
    "                        except Exception as e2:\n",
    "                            print(\"Exception while saving occurred:\", e2)\n",
    "\n",
    "                print(\"Duplicates found during Scraping:\", n_duplicates_found)\n",
    "                return result_df\n",
    "            \n",
    "            except PermissionError as e:\n",
    "                \n",
    "                print(\"Main Loop Permission Error:\", e)\n",
    "                sleeping_time_after_permission_error = 180\n",
    "                \n",
    "                print(\"\\n\")\n",
    "                print(f\"Sleeping for {sleeping_time_after_permission_error} seconds, to wait for cloud update!\")\n",
    "                time.sleep(sleeping_time_after_permission_error)\n",
    "                print(\"\\n\")\n",
    "                \n",
    "                print(\"CURRENTLY SAVING ALL, DONT STOP!\")\n",
    "                logging_df.to_csv(f\"logging/logging_data/logging_df_{user}.csv\", index=False)\n",
    "                result_df.to_csv(in_out_path, index=False)\n",
    "                print(\"SAVING DONE!\")\n",
    "                print(\"Saved Result and Logging DF and rotating VPN!\")\n",
    "                print(\"\\n\")\n",
    "                rotate_VPN(instructions_vpn)\n",
    "            \n",
    "            except Exception as e:\n",
    "\n",
    "                print(\"Main Loop Exception:\", e)\n",
    "                print(\"\")  \n",
    "\n",
    "                print(\"CURRENTLY SAVING ALL, DONT STOP!\")\n",
    "                logging_df.to_csv(f\"logging/logging_data/logging_df_{user}.csv\", index=False)\n",
    "                result_df.to_csv(in_out_path, index=False)\n",
    "                print(\"SAVING DONE!\")\n",
    "                print(\"Saved Result and Logging DF and rotating VPN!\")\n",
    "                rotate_VPN(instructions_vpn)\n",
    "\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"SAVING ALL DUE TO KEYBOARD INTERRUPT!\")\n",
    "        logging_df.to_csv(f\"logging/logging_data/logging_df_{user}.csv\", index=False)\n",
    "        result_df.to_csv(in_out_path, index=False)\n",
    "        print(\"SAVING DONE!\")\n",
    "        \n",
    "    except PermissionError: \n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"SAVING ALL DUE PERMISSION ERROR!\")\n",
    "        logging_df.to_csv(f\"logging/logging_data/logging_df_{user}.csv\", index=False)\n",
    "        result_df.to_csv(in_out_path, index=False)\n",
    "        print(\"SAVING DONE!\")\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"SAVING ALL DUE TO UNSEEN EXCEPTION!\")\n",
    "        logging_df.to_csv(f\"logging/logging_data/logging_df_{user}.csv\", index=False)\n",
    "        result_df.to_csv(in_out_path, index=False)\n",
    "        print(\"SAVING DONE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c019bc3",
   "metadata": {},
   "source": [
    "## Optimized Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4cd041",
   "metadata": {},
   "source": [
    "### Base Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043877c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(logging_df, result_df, user, in_out_path):\n",
    "    # Save logging and result data\n",
    "    logging_df.to_csv(f\"logging/logging_data/logging_df_{user}.csv\", index=False)\n",
    "    result_df.to_csv(in_out_path, index=False)\n",
    "    print(\"Data saved successfully!\")\n",
    "    print(\"\")\n",
    "\n",
    "def scrape_offers(doc):\n",
    "    # Extract offer URLs from the parsed HTML document\n",
    "    return [\n",
    "        \"https://www.autoscout24.de\" + a.get(\"href\")\n",
    "        for a in doc.find_all(\"a\")\n",
    "        if '/angebote/' in str(a.get(\"href\")) and '/leasing/' not in str(a.get(\"href\")) and '/recommendation/' not in str(a.get(\"href\"))\n",
    "    ]\n",
    "\n",
    "def extract_car_data(doc, url):\n",
    "    # Extract car data from the parsed HTML document\n",
    "    car_data = {}\n",
    "    for key, value in zip(doc.find_all(\"dt\"), doc.find_all(\"dd\")):\n",
    "        if key.text not in attribute_exception_list:\n",
    "            car_data[key.text.strip()] = value.text.strip()\n",
    "    car_data.update({\n",
    "        \"url\": url,\n",
    "        \"date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "        \"time\": datetime.now().strftime(\"%H-%M-%S\"),\n",
    "        \"model\": doc.find(\"span\", class_=\"StageTitle_model__EbfjC\").get_text(),\n",
    "        \"brand\": doc.find(\"span\", class_=\"StageTitle_boldClassifiedInfo__sQb0l\").get_text()\n",
    "    })\n",
    "    return car_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb673be4",
   "metadata": {},
   "source": [
    "### Main Scraping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fb3174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to perform web scraping with optimization\n",
    "def webscrape_loop_optimized(user, sleep_interval, attribute_exception_list, instructions_vpn, in_out_path,\n",
    "                   n_tries=10, max_pages = 20, do_backup = False, adage = 7, use_recency = False, print_duplicate_url = False):\n",
    "\n",
    "    n_duplicates = 0\n",
    "    try:\n",
    "        for _ in range(n_tries):\n",
    "            try:\n",
    "                # Load existing data\n",
    "                try:\n",
    "                    result_df = pd.read_csv(in_out_path, low_memory=False)\n",
    "                    all_used_urls = set(result_df[\"url\"])\n",
    "                    print(f\"Loaded {len(result_df)} entries.\\n\")\n",
    "                except FileNotFoundError:\n",
    "                    result_df = pd.DataFrame()\n",
    "                    all_used_urls = set()\n",
    "\n",
    "                # Load logging data\n",
    "                logging_df = pd.read_csv(f\"logging/logging_data/logging_df_{user}.csv\")\n",
    "                logging_df_short = logging_df[logging_df[\"user\"] == user].copy()\n",
    "\n",
    "                for _, row in logging_df_short.iterrows():\n",
    "                    curr_brand = str(row[\"brand\"])\n",
    "                    curr_model = str(row[\"model\"])\n",
    "\n",
    "                    # Rotate VPN if necessary\n",
    "                    if int(row[\"curr_year\"]) < row[\"end_year\"]:\n",
    "                        rotate_VPN(instructions_vpn)\n",
    "\n",
    "                    for curr_year in range(int(row[\"curr_year\"]), int(row[\"end_year\"]) + 1):\n",
    "                        n_duplicates = 0  # Reset duplicate count for each year\n",
    "\n",
    "                        for curr_page in range(1, max_pages + 1):\n",
    "                            \n",
    "                            print(f\"\\nBrand: {curr_brand}\\nModel: {curr_model}\\nYear: {curr_year}\\nPage: {curr_page}\\n\")\n",
    "                            print(f\"Current Amount of total Entries: {len(result_df)}!\")\n",
    "                            print(\"Current Time:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "  \n",
    "                            # Generate the URL based on the given parameters\n",
    "                            if use_recency:\n",
    "                                model_url = f\"https://www.autoscout24.de/lst/{curr_brand}/{curr_model}/re_{curr_year}?adage={adage}&atype=C&cy=D&damaged_listing=exclude&desc=0&ocs_listing=include&page={curr_page}&powertype=kw&search_id=ehnjs4dnm6&sort=standard&source=listpage_pagination\"\n",
    "                            else:\n",
    "                                model_url = f\"https://www.autoscout24.de/lst/{curr_brand}/{curr_model}/re_{curr_year}?atype=C&cy=D&damaged_listing=exclude&desc=0&ocs_listing=include&page={curr_page}&powertype=kw&search_id=ehnjs4dnm6&sort=standard&source=listpage_pagination\"\n",
    "\n",
    "                            response = requests.get(model_url)\n",
    "                            doc = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                            # Extract number of offers\n",
    "                            try:\n",
    "                                no_of_offers = int(doc.find('div', class_='ListHeader_title_with_sort__Pf4Zw').find('span').text.strip().split()[0].replace(\".\", \"\"))\n",
    "                                print(\"Number of total offers:\", no_of_offers)\n",
    "                            except AttributeError:\n",
    "                                no_of_offers = 0\n",
    "                                print(\"Failed to extract number of offers.\")\n",
    "\n",
    "                            # If no offers, break the loop\n",
    "                            if no_of_offers == 0:\n",
    "                                time.sleep(random.uniform(*sleep_interval))\n",
    "                                print(\"No offers for this year, continuing with next year!\")\n",
    "                                break\n",
    "\n",
    "                            time.sleep(random.uniform(*sleep_interval))\n",
    "\n",
    "                            # Scrape offer URLs\n",
    "                            offer_list = scrape_offers(doc)\n",
    "                            print(f\"Found {len(offer_list)} offers on this page.\")\n",
    "                            print(\"\")\n",
    "\n",
    "                            if len(offer_list) == 0:\n",
    "                                print(\"Last page reached!\")\n",
    "                                print(\"\")\n",
    "                                break\n",
    "\n",
    "                            # Process each offer\n",
    "                            for item in offer_list:\n",
    "                                try:\n",
    "                                    if item not in all_used_urls:\n",
    "                                        time.sleep(random.uniform(*sleep_interval))\n",
    "                                        response = requests.get(item)\n",
    "                                        doc = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                                        # Extract car data and update result dataframe\n",
    "                                        car_data = extract_car_data(doc, item)\n",
    "                                        \n",
    "                                                                                \n",
    "                                        ################## Janik: New Part ########################\n",
    "                                        \n",
    "                                        # Check if the column exists, if not, add it to the beginning\n",
    "                                        if 'Barzahlungspreis' not in car_data:\n",
    "                                            price_information =  re.split(r'(?<=-)', doc.find('div', class_='PriceInfo_wrapper__hreB_').find('span', class_='PriceInfo_price__XU0aF').text.strip())[0]\n",
    "                                            car_data = {'Barzahlungspreis':  price_information, **car_data}\n",
    "\n",
    "                                        #############################################################\n",
    "                                        \n",
    "                                        \n",
    "                                        \n",
    "                                        result_df = pd.concat([result_df, pd.DataFrame([car_data])], ignore_index=True)\n",
    "                                        all_used_urls.add(item)\n",
    "                                    else:\n",
    "                                        n_duplicates += 1\n",
    "                                        if print_duplicate_url:\n",
    "                                    \n",
    "                                            print(\"Duplicate found:\", item)\n",
    "                                        \n",
    "                                except Exception as e:\n",
    "                                    print(\"Error accessing car URL:\", e)\n",
    "                                    save_data(logging_df, result_df, user, in_out_path)\n",
    "                                    rotate_VPN(instructions_vpn)\n",
    "                                    break\n",
    "\n",
    "                            if len(offer_list) <= 5:\n",
    "                                print(\"Last page reached!\")\n",
    "                                break\n",
    "\n",
    "                        # Update logging data\n",
    "                        logging_df.loc[(logging_df[\"user\"] == user) & (logging_df[\"brand\"] == curr_brand) & (logging_df[\"model\"] == curr_model), \"curr_year\"] = curr_year + 1\n",
    "                        logging_df.loc[(logging_df[\"user\"] == user) & (logging_df[\"brand\"] == curr_brand) & (logging_df[\"model\"] == curr_model), \"last_scraped\"] = datetime.now().strftime(\"%Y-%m-%d %H %M\")\n",
    "\n",
    "                        print(f\"Offers for {curr_year}: {no_of_offers}, Duplicates: {n_duplicates}\")\n",
    "                        \n",
    "                        if no_of_offers - n_duplicates > 0:\n",
    "                            \n",
    "                            try:\n",
    "                                print(\"\")\n",
    "                                print(\"CURRENTLY SAVING RESULTS, DONT STOP!\")\n",
    "                                result_df.to_csv(in_out_path, index=False)\n",
    "                                print(\"SAVING DONE!\")\n",
    "                                \n",
    "                            except KeyboardInterrupt:\n",
    "                                \n",
    "                                print(\"Interrupted by user. Saving data...\")\n",
    "                                save_data(logging_df, result_df, user, in_out_path)\n",
    "\n",
    "                        # Save progress\n",
    "                        try:\n",
    "                            print(\"\")\n",
    "                            print(\"CURRENTLY SAVING LOGGING_DF, DONT STOP!\")\n",
    "                            logging_df.to_csv(f\"logging/logging_data/logging_df_{user}.csv\", index=False)\n",
    "                            print(\"SAVING DONE!\")\n",
    "                            print(\"\")\n",
    "\n",
    "                        except KeyboardInterrupt:\n",
    "\n",
    "                            print(\"Permission Error. Saving data...\")\n",
    "                            save_data(logging_df, result_df, user, in_out_path)\n",
    "                            \n",
    "\n",
    "                        if do_backup:\n",
    "                            backup_path = f\"scraped_data/{user}/backups/{user}_data_{datetime.now().strftime('%Y-%m-%d %H-%M')}.csv\"\n",
    "                            result_df.to_csv(backup_path)\n",
    "                            print(f\"Backup saved to {backup_path}\")\n",
    "                            \n",
    "                        print(\"##################################################################\")\n",
    "                        print(\"\")\n",
    "\n",
    "                print(\"Scraping completed. Duplicates found:\", n_duplicates)\n",
    "                return result_df\n",
    "\n",
    "            except PermissionError as e:\n",
    "                print(\"Permission Error:\", e)\n",
    "                print(\"Waiting for cloud update...\")\n",
    "                time.sleep(180)\n",
    "                save_data(logging_df, result_df, user, in_out_path)\n",
    "                rotate_VPN(instructions_vpn)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Exception:\", e)\n",
    "                save_data(logging_df, result_df, user, in_out_path)\n",
    "                rotate_VPN(instructions_vpn)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\")\n",
    "        print(\"Interrupted by user. Saving data...\")\n",
    "        save_data(logging_df, result_df, user, in_out_path)\n",
    "\n",
    "    except PermissionError:\n",
    "        print(\"\")\n",
    "        print(\"Permission Error. Saving data...\")\n",
    "        save_data(logging_df, result_df, user, in_out_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\")        \n",
    "        print(\"Unexpected Exception:\", e)\n",
    "        save_data(logging_df, result_df, user, in_out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c30828a",
   "metadata": {},
   "source": [
    "# Concat Scraped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a5d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scraped_data(user_list, project_location):\n",
    "\n",
    "    scraped_data = pd.DataFrame()\n",
    "\n",
    "    for user in user_list:\n",
    "\n",
    "        input_path = f\"{project_location}scraped_data\\\\{user}\\\\{user}_data.csv\"\n",
    "\n",
    "\n",
    "        try:\n",
    "            user_df = pd.read_csv(input_path, low_memory = False)\n",
    "            print(\"Data from\", user, \"succesfully loaded with\", len(user_df), \"entries!\")\n",
    "\n",
    "\n",
    "        except FileNotFoundError:\n",
    "\n",
    "            user_df = pd.DataFrame()\n",
    "\n",
    "        scraped_data = pd.concat([scraped_data, user_df])\n",
    "\n",
    "    initial_length = len(scraped_data)\n",
    "    print(\"\")\n",
    "    print(\"Concatenated DF created with\", initial_length, \"entries!\")\n",
    "    \n",
    "    return scraped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e17b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unnamed_columns(df):\n",
    "    \n",
    "    df1 = df.copy()\n",
    "    # List comprehension to get column names containing \"Unnamed\" \n",
    "    unnamed_columns = [col for col in df.columns if 'Unnamed' in col]\n",
    "    \n",
    "    print(\"Dropping \" + str(len(unnamed_columns)) + \" columns:\\n\" + str(unnamed_columns))\n",
    "    # Drop the unnamed columns\n",
    "    df1 = df1.drop(columns=unnamed_columns, errors = 'ignore')\n",
    "    \n",
    "    return df1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
